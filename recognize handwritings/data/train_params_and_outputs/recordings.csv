time,loss,epoch,batch_size,accuracy,train_time
Tue Mar  7 15:56:04 2023,"tensor(2.2455, grad_fn=<NllLossBackward0>)",,,,
Wed Mar  8 11:12:15 2023,"tensor(1.8787, grad_fn=<NllLossBackward0>)",1,800,,
,,,,,
Wed Mar  8 11:15:53 2023,"tensor(1.2343, grad_fn=<NllLossBackward0>)",1,800,,
,,,,,
Wed Mar  8 11:35:59 2023,"tensor(0.4046, grad_fn=<NllLossBackward0>)",1,800,,
,,,,,
Fri Mar 10 11:02:26 2023,"tensor(0.3728, grad_fn=<NllLossBackward0>)",1,800,,
Fri Mar 10 11:10:08 2023,"tensor(0.4103, grad_fn=<NllLossBackward0>)",1,800,0.883,
,,,,,
Fri Mar 10 11:37:06 2023,"tensor(0.4674, grad_fn=<NllLossBackward0>)",1,800,0.8686,
,,,,,
Fri Mar 10 11:40:53 2023,"tensor(0.2688, grad_fn=<NllLossBackward0>)",10,800,0.9171,
Fri Mar 10 12:00:54 2023,"tensor(0.4035, grad_fn=<NllLossBackward0>)",10,800,0.9063,59.33679151535034
Tue Mar 14 14:54:18 2023,"tensor(0.2914, grad_fn=<NllLossBackward0>)",10,800,0.9188,88.81521415710449
Tue Mar 14 14:57:03 2023,0.270369291305542,10,800,0.9212,87.72912502288818
Wed Mar 15 15:50:25 2023,nan,10,800,0.098,87.92238664627075
